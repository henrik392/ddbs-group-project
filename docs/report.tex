\documentclass[12pt,a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Code listing style
\lstset{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    urlcolor=blue,
}

% Header/Footer
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{Distributed Database System for News Article Management}

% Title formatting
\titleformat{\section}{\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{1em}{}

% Document
\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace*{2cm}

    {\Huge\bfseries Distributed Database System for\\News Article Management\par}
    \vspace{1.5cm}

    {\Large Project Report\par}
    \vspace{1cm}

    {\large Distributed Database Systems Course\par}
    \vspace{0.5cm}
    {\large Tsinghua University\par}
    \vspace{2cm}

    {\large\itshape Henrik Kvamme\par}
    {\large\itshape Rui Silveira\par}
    \vspace{1cm}

    {\large \today\par}

    \vfill
\end{titlepage}

% Abstract
\begin{abstract}
This project implements a distributed database management system for a news article platform using horizontal fragmentation and replication across multiple database nodes. The system manages both structured data (User, Article, Read, Be-Read, Popular-Rank tables) and unstructured data (text, images, video) using PostgreSQL for relational data, Redis for query caching, and MinIO for object storage. We implement a coordinator-based architecture that handles query routing, distributed joins, result merging, and automatic failover with hot-cold standby. The system successfully demonstrates horizontal data partitioning, selective replication, distributed query processing, fault tolerance, and comprehensive monitoring capabilities. Performance evaluation shows effective load distribution, query caching with hit rates exceeding 15\%, and zero-downtime failover.

\vspace{0.5cm}
\noindent\textbf{Keywords:} Distributed databases, horizontal fragmentation, query coordination, replication, fault tolerance, caching
\end{abstract}

\newpage
\tableofcontents
\newpage

% Main content
\section{Introduction}

\subsection{Problem Background and Motivation}

Modern news platforms face significant challenges in managing massive amounts of data distributed across geographic regions. Traditional centralized database systems exhibit critical limitations:

\begin{itemize}
    \item \textbf{Scalability:} Single-node systems cannot efficiently handle growing data volumes and concurrent user requests
    \item \textbf{Availability:} Single point of failure compromises entire system reliability
    \item \textbf{Performance:} Geographic distribution causes high latency for distant users
    \item \textbf{Data Locality:} Users accessing data from different regions experience variable performance
    \item \textbf{Fault Tolerance:} Hardware failures can cause prolonged downtime
\end{itemize}

Distributed database systems address these challenges by partitioning data across multiple nodes, replicating critical data for availability, processing queries in parallel, and implementing automatic failover mechanisms.

\subsection{Objectives}

This project aims to implement a distributed database system that:

\begin{enumerate}
    \item Manages structured relational data (5 tables) and unstructured data (text files, images, videos)
    \item Implements horizontal fragmentation based on business logic (region, category, temporal granularity)
    \item Provides selective replication for high availability and data locality
    \item Supports efficient distributed query processing with intelligent routing
    \item Implements hot-cold standby for fault tolerance with automatic failover
    \item Provides query result caching using Redis
    \item Monitors system health, data distribution, and workload statistics
\end{enumerate}

\section{Related Work}

\subsection{Distributed Database Technologies}

\textbf{MongoDB Sharding:}
MongoDB provides automatic horizontal scaling through sharding with built-in replication and load balancing. However, its document-oriented model lacks relational database features and ACID guarantees across shards.

\textbf{MySQL Cluster:}
MySQL Cluster offers synchronous replication in a shared-nothing architecture with automatic partitioning. The complexity of configuration and limited flexibility in fragmentation rules make it less suitable for custom requirements.

\textbf{PostgreSQL with Citus:}
Citus extends PostgreSQL with distributed capabilities, providing horizontal scaling while maintaining SQL compatibility. However, it requires the Citus extension and abstracts away fragmentation control.

\textbf{Apache Cassandra:}
Cassandra provides wide-column storage with tunable consistency and high availability. Its eventual consistency model and lack of full ACID compliance make it unsuitable for applications requiring strong consistency.

\subsection{Our Approach}

We implement a \textbf{custom coordinator-based architecture} using:
\begin{itemize}
    \item PostgreSQL for ACID compliance and full relational model support
    \item Redis for high-performance distributed caching
    \item MinIO for scalable object storage
    \item Python-based coordinator for flexible query routing and execution
\end{itemize}

This approach provides complete control over fragmentation rules, transparent query processing, simpler deployment without proprietary extensions, and clear understanding of distributed database concepts.

\section{Problem Definition}

\subsection{Data Model}

The system manages data for a news article platform with the following schema:

\subsubsection{Structured Tables}

\begin{enumerate}
    \item \textbf{User} (13 attributes): uid, timestamp, name, gender, email, phone, department, grade, language, region, role, preferTags, obtainedCredits
    \item \textbf{Article} (12 attributes): aid, timestamp, title, category, abstract, articleTags, authors, language, text, image, video
    \item \textbf{Read} (9 attributes): id, timestamp, uid, aid, readTimeLength, readSequence, agreeOrNot, commentOrNot, shareOrNot, commentDetail
    \item \textbf{Be-Read} (11 attributes): id, timestamp, aid, readNum, readUidList, commentNum, commentUidList, agreeNum, agreeUidList, shareNum, shareUidList
    \item \textbf{Popular-Rank} (4 attributes): id, timestamp, temporalGranularity (daily/weekly/monthly), articleAidList (top-5 comma-separated)
\end{enumerate}

\subsubsection{Unstructured Data}

\begin{itemize}
    \item Article text files (plain text format)
    \item Images: 1-5 JPEG/PNG images per article
    \item Videos: 5\% of articles include FLV video files
\end{itemize}

\subsection{Fragmentation Strategy}

We implement horizontal fragmentation with selective replication based on business logic:

\begin{table}[h]
\centering
\caption{Horizontal Fragmentation Rules}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Table} & \textbf{Attribute} & \textbf{Distribution} \\ \midrule
User & region & Beijing $\rightarrow$ DBMS1, Hong Kong $\rightarrow$ DBMS2 \\
Article & category & science $\rightarrow$ DBMS1 \& DBMS2 (replicated) \\
 &  & technology $\rightarrow$ DBMS2 only \\
Read & Co-located & Beijing users' reads $\rightarrow$ DBMS1 \\
 & with User & Hong Kong users' reads $\rightarrow$ DBMS2 \\
Be-Read & category & science $\rightarrow$ both DBMS (replicated) \\
 &  & technology $\rightarrow$ DBMS2 only \\
Popular-Rank & temporal & daily $\rightarrow$ DBMS1 \\
 & Granularity & weekly, monthly $\rightarrow$ DBMS2 \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Rationale:}
\begin{itemize}
    \item \textbf{User/Read co-location:} Minimizes distributed joins for user reading history queries
    \item \textbf{Science article replication:} Ensures high availability for popular content
    \item \textbf{Category-based distribution:} Balances load based on article types
    \item \textbf{Temporal granularity:} Distributes ranking computation workload
\end{itemize}

\subsection{Functional Requirements}

\begin{enumerate}
    \item \textbf{Data Loading:} Bulk load structured data from SQL files with automatic partitioning according to fragmentation rules
    \item \textbf{Query Processing:}
    \begin{itemize}
        \item Query users and articles with filtering conditions
        \item Support distributed queries spanning multiple DBMS
        \item Merge results maintaining correct ordering
    \end{itemize}
    \item \textbf{Be-Read Population:} Aggregate Read data from both DBMS, compute statistics, and distribute according to replication rules
    \item \textbf{Popular-Rank Queries:} Retrieve top-5 popular articles with complete details using distributed join (Popular-Rank + Article)
    \item \textbf{Fault Tolerance:} Hot-cold standby for DBMS1 with automatic failover
    \item \textbf{Caching:} Query result caching with Redis
    \item \textbf{Monitoring:} DBMS health status, data distribution verification, query workload statistics
\end{enumerate}

\section{System Design and Implementation}

\subsection{System Architecture}

Figure \ref{fig:architecture} shows the overall system architecture.

\begin{figure}[h]
\centering
\begin{verbatim}
┌────────────────────────────────────────────────┐
│         Coordinator (Python)                   │
│   • Query parsing & routing                    │
│   • Query execution & result merging           │
│   • Cache management (Redis)                   │
│   • Failover detection & recovery              │
└──────────┬─────────────────────────────────────┘
           │
    ┌──────┴────────┐
    ↓               ↓
┌─────────┐    ┌──────────┐    ┌─────────────┐
│ DBMS1   │←──→│DBMS1-    │    │   DBMS2     │
│(Beijing)│    │STANDBY   │    │(Hong Kong)  │
│:5434    │    │:5435     │    │   :5433     │
└────┬────┘    └──────────┘    └──────┬──────┘
     │                                 │
     └─────────────┬───────────────────┘
                   ↓
            ┌─────────────┐
            │ Redis Cache │
            │   :6379     │
            └──────┬──────┘
                   ↓
            ┌─────────────┐
            │    MinIO    │
            │ :9000/9001  │
            └─────────────┘
\end{verbatim}
\caption{System Architecture}
\label{fig:architecture}
\end{figure}

\subsection{Components}

\subsubsection{Database Layer (PostgreSQL)}

Three PostgreSQL 16 instances provide the distributed storage layer:

\begin{itemize}
    \item \textbf{DBMS1} (port 5434): Primary instance for Beijing region data and daily rankings
    \item \textbf{DBMS1-STANDBY} (port 5435): Hot standby replica for fault tolerance
    \item \textbf{DBMS2} (port 5433): Hong Kong region data and weekly/monthly rankings
\end{itemize}

Each instance includes optimized indexes:
\begin{itemize}
    \item \texttt{idx\_user\_region} on User(region) for regional queries
    \item \texttt{idx\_article\_category} on Article(category) for category-based routing
    \item \texttt{idx\_read\_uid} on Read(uid) for user-based aggregation
    \item \texttt{idx\_beread\_aid} on Be-Read(aid) for article statistics lookup
\end{itemize}

\subsubsection{Cache Layer (Redis)}

Redis provides high-performance distributed caching:
\begin{itemize}
    \item \textbf{Cache Key:} MD5 hash of SQL query string
    \item \textbf{TTL:} 60 seconds for query results
    \item \textbf{Storage:} JSON-serialized result sets
    \item \textbf{Metrics:} Hit rate tracking and workload statistics
\end{itemize}

\subsubsection{Storage Layer (MinIO)}

MinIO object storage handles unstructured data:
\begin{itemize}
    \item Article text files in \texttt{article-text} bucket
    \item Images in \texttt{article-images} bucket (JPEG/PNG)
    \item Videos in \texttt{article-videos} bucket (FLV format)
\end{itemize}

\subsubsection{Coordinator Layer}

The coordinator implements three key modules:

\textbf{Query Router} (\texttt{src/domains/query/router.py}):
\begin{enumerate}
    \item Parses SQL query using regex to extract table and conditions
    \item Identifies fragmentation attribute and values
    \item Applies fragmentation rules to determine target DBMS
    \item Returns routing plan with strategy (single/parallel/join)
\end{enumerate}

\textbf{Query Executor} (\texttt{src/domains/query/executor.py}):
\begin{enumerate}
    \item Executes queries on target DBMS with connection pooling
    \item Implements three execution strategies:
    \begin{itemize}
        \item \textbf{Single:} Execute on one DBMS only
        \item \textbf{Parallel:} Execute on multiple DBMS concurrently, merge results
        \item \textbf{Join:} Distributed join for Popular-Rank + Article
    \end{itemize}
    \item Handles failover to standby on connection failure
    \item Merges and deduplicates results maintaining order
\end{enumerate}

\textbf{Query Coordinator} (\texttt{src/domains/query/coordinator.py}):
\begin{enumerate}
    \item Checks Redis cache for query result
    \item On cache miss: routes query via Router, executes via Executor
    \item Caches result with 60s TTL
    \item Updates cache statistics
\end{enumerate}

\subsection{Key Algorithms}

\subsubsection{Query Routing Algorithm}

\begin{lstlisting}[language=Python, caption=Query Routing Algorithm]
def route_query(sql: str) -> RoutingPlan:
    table = extract_table(sql)
    conditions = parse_where_clause(sql)

    # Apply fragmentation rules
    if table == "user":
        if "region='Beijing'" in conditions:
            return RoutingPlan(strategy="single", targets=[DBMS1])
        elif "region='Hong Kong'" in conditions:
            return RoutingPlan(strategy="single", targets=[DBMS2])
        else:
            return RoutingPlan(strategy="parallel", targets=[DBMS1, DBMS2])

    elif table == "article":
        if "category='science'" in conditions:
            # Science articles replicated on both DBMS
            return RoutingPlan(strategy="parallel", targets=[DBMS1, DBMS2])
        elif "category='technology'" in conditions:
            return RoutingPlan(strategy="single", targets=[DBMS2])
        else:
            return RoutingPlan(strategy="parallel", targets=[DBMS1, DBMS2])

    # Similar logic for other tables...
    return RoutingPlan(strategy="parallel", targets=[DBMS1, DBMS2])
\end{lstlisting}

\subsubsection{Distributed Join Algorithm}

The top-5 popular articles query requires a distributed join between Popular-Rank and Article tables:

\begin{lstlisting}[language=Python, caption=Distributed Join for Top-5 Articles]
def get_top5_articles(granularity: str) -> List[Article]:
    # Step 1: Determine ranking location
    rank_dbms = DBMS1 if granularity == "daily" else DBMS2

    # Step 2: Fetch ranking
    query = f"""
        SELECT articleaidlist FROM "popular_rank"
        WHERE temporalgranularity='{granularity}'
        ORDER BY timestamp DESC LIMIT 1
    """
    ranking = execute_query(rank_dbms, query)

    # Step 3: Extract article IDs (comma-separated)
    aid_list = ranking[0]['articleaidlist'].split(',')
    aids = ','.join(aid_list)

    # Step 4: Fetch article details from both DBMS
    article_query = f"""
        SELECT * FROM "article" WHERE aid IN ({aids})
    """
    articles_dbms1 = execute_query(DBMS1, article_query)
    articles_dbms2 = execute_query(DBMS2, article_query)

    # Step 5: Merge and maintain ranking order
    all_articles = {a['aid']: a for a in articles_dbms1 + articles_dbms2}
    ordered_articles = [all_articles[aid] for aid in aid_list if aid in all_articles]

    return ordered_articles
\end{lstlisting}

\subsubsection{Failover Algorithm}

The failover mechanism provides automatic recovery when DBMS1 fails:

\begin{lstlisting}[language=Python, caption=Automatic Failover Algorithm]
def execute_with_failover(dbms: str, sql: str) -> List[Dict]:
    try:
        # Try primary DBMS
        return execute_query(dbms, sql)
    except ConnectionError as e:
        if dbms == DBMS1:
            # Automatic failover to standby
            print("Warning: DBMS1 failed, trying standby DBMS1-STANDBY")
            return execute_query(DBMS1_STANDBY, sql)
        else:
            raise e
\end{lstlisting}

\subsubsection{Be-Read Population Algorithm}

Be-Read table aggregates read statistics from both DBMS:

\begin{lstlisting}[language=Python, caption=Be-Read Population with Replication]
def populate_beread():
    # Step 1: Fetch all reads from both DBMS
    reads_dbms1 = fetch_all_reads(DBMS1)
    reads_dbms2 = fetch_all_reads(DBMS2)
    all_reads = reads_dbms1 + reads_dbms2

    # Step 2: Aggregate by article
    aggregated = defaultdict(lambda: {
        'readNum': 0, 'readUidList': [],
        'commentNum': 0, 'commentUidList': [],
        'agreeNum': 0, 'agreeUidList': [],
        'shareNum': 0, 'shareUidList': []
    })

    for read in all_reads:
        stats = aggregated[read['aid']]
        stats['readNum'] += 1
        stats['readUidList'].append(read['uid'])
        if read['agreeoornot'] == 1:
            stats['agreeNum'] += 1
            stats['agreeUidList'].append(read['uid'])
        # Similar for comments and shares...

    # Step 3: Insert with replication rules
    for aid, stats in aggregated.items():
        article_category = get_article_category(aid)

        if article_category == 'science':
            # Replicate science articles to both DBMS
            insert_beread(DBMS1, aid, stats)
            insert_beread(DBMS2, aid, stats)
        else:
            # Technology articles only on DBMS2
            insert_beread(DBMS2, aid, stats)
\end{lstlisting}

\subsection{Data Loading Process}

Data loading implements automatic partitioning during generation:

\begin{enumerate}
    \item Generate test data using \texttt{db-generation/generate\_test\_data.py}
    \item Partition data according to fragmentation rules:
    \begin{itemize}
        \item Users: Split by region into separate SQL files
        \item Articles: Replicate science articles, partition technology
        \item Reads: Co-locate with corresponding users
    \end{itemize}
    \item Generate SQL INSERT statements for each DBMS
    \item Bulk load using \texttt{psycopg} COPY command for performance
    \item Verify distribution with monitoring tools
\end{enumerate}

\section{Evaluation and Results}

\subsection{System Deployment}

Figure \ref{fig:docker} shows the deployed system with all components running.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/04_docker_ps.png}
\caption{System deployment showing all distributed components operational including hot standby}
\label{fig:docker}
\end{figure}

The system successfully deploys with:
\begin{itemize}
    \item 3 PostgreSQL instances (DBMS1, DBMS1-STANDBY, DBMS2)
    \item 1 Redis instance for caching
    \item 1 MinIO instance for object storage
\end{itemize}

All services are containerized using Docker Compose for reproducible deployment.

\subsection{Data Distribution Verification}

Figure \ref{fig:distribution} shows the data distribution across DBMS instances.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/05_distribution.png}
\caption{Data distribution across DBMS1 and DBMS2 showing correct horizontal fragmentation}
\label{fig:distribution}
\end{figure}

\textbf{Distribution Results:}

\begin{table}[h]
\centering
\caption{Data Distribution Verification}
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Table} & \textbf{DBMS1} & \textbf{DBMS2} & \textbf{Total} \\ \midrule
User & 70 (Beijing) & 30 (Hong Kong) & 100 \\
Article & 25 (science) & 50 (science + tech) & 50 unique \\
Read & 353 (Beijing users) & 147 (HK users) & 500 \\
Be-Read & 25 (science) & 50 (all articles) & 50 unique \\
Popular-Rank & 1 (daily) & 2 (weekly, monthly) & 3 \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Verification:}
\begin{itemize}
    \item User table correctly partitioned by region (70\% Beijing, 30\% Hong Kong)
    \item Science articles properly replicated on both DBMS
    \item Technology articles only on DBMS2
    \item Read table co-located with User table
    \item Be-Read replication follows article category rules
    \item Popular-Rank distributed by temporal granularity
\end{itemize}

\textbf{Result:} Fragmentation rules correctly implemented ✓

\subsection{Query Performance}

\subsubsection{Distributed Query Execution}

Figure \ref{fig:query} shows query execution merging results from both DBMS.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/07_query_execution.png}
\caption{Distributed query execution merging results from both DBMS}
\label{fig:query}
\end{figure}

\subsubsection{Query Caching Performance}

Figure \ref{fig:cache} demonstrates cache performance with repeated queries.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/08_cache_hit.png}
\caption{Query cache demonstration: first execution vs. cached execution}
\label{fig:cache}
\end{figure}

\textbf{Performance Results:}

\begin{table}[h]
\centering
\caption{Query Performance Metrics}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Query Type} & \textbf{Response Time} & \textbf{Notes} \\ \midrule
Single DBMS (Beijing users) & \textasciitilde50ms & Routed to DBMS1 only \\
Distributed (All users) & \textasciitilde80ms & Parallel execution + merge \\
First execution & \textasciitilde60ms & Cache miss, full execution \\
Cached execution & \textasciitilde5ms & \textbf{92\% reduction} \\
Cache hit rate & 16.67\% & After moderate workload \\ \bottomrule
\end{tabular}
\end{table}

\textbf{Analysis:}
\begin{itemize}
    \item Single DBMS queries show low latency (\textasciitilde50ms)
    \item Distributed queries maintain acceptable performance (\textasciitilde80ms)
    \item Caching provides 92\% response time reduction
    \item Cache hit rate exceeds 15\% target
\end{itemize}

\subsection{Distributed Join}

Figure \ref{fig:top5} shows the top-5 popular articles query with complete article details.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/09_top5_articles.png}
\caption{Top-5 daily popular articles with distributed join across DBMS}
\label{fig:top5}
\end{figure}

\textbf{Distributed Join Verification:}
\begin{itemize}
    \item Correctly fetches ranking from appropriate DBMS (daily $\rightarrow$ DBMS1)
    \item Queries article details from both DBMS1 and DBMS2
    \item Merges results maintaining ranking order
    \item Returns complete article information (title, category, abstract, text, image, video)
    \item Handles articles distributed across both DBMS transparently
\end{itemize}

\textbf{Result:} Distributed join successful ✓

\subsection{Fault Tolerance and High Availability}

\subsubsection{Hot-Cold Standby Implementation}

Figure \ref{fig:standby} shows the system status with hot standby configured.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/01_standby_status.png}
\caption{System status showing primary DBMS1 and hot standby DBMS1-STANDBY both online}
\label{fig:standby}
\end{figure}

The hot-cold standby architecture provides fault tolerance for DBMS1:
\begin{itemize}
    \item \textbf{Primary:} DBMS1 (port 5434) handles all normal queries
    \item \textbf{Standby:} DBMS1-STANDBY (port 5435) maintains synchronized replica
    \item \textbf{Synchronization:} Data loaded identically to both instances
    \item \textbf{Monitoring:} Health checks on both primary and standby
\end{itemize}

\subsubsection{Automatic Failover Demonstration}

Figures \ref{fig:failover-before} and \ref{fig:failover-during} demonstrate automatic failover behavior.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/02_failover_before.png}
\caption{Query execution on primary DBMS1 under normal operation}
\label{fig:failover-before}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/03_failover_during.png}
\caption{Automatic failover to DBMS1-STANDBY when primary DBMS1 fails}
\label{fig:failover-during}
\end{figure}

\textbf{Failover Test Procedure:}
\begin{enumerate}
    \item Execute query on healthy DBMS1 (Figure \ref{fig:failover-before})
    \item Stop primary DBMS1 container: \texttt{docker stop dbms1}
    \item Execute same query (Figure \ref{fig:failover-during})
    \item System detects DBMS1 failure
    \item Automatically routes query to DBMS1-STANDBY
    \item Returns identical results with warning message
    \item Restart DBMS1: \texttt{docker start dbms1}
    \item System automatically uses primary again
\end{enumerate}

\textbf{Failover Results:}
\begin{itemize}
    \item \textbf{Detection time:} \textasciitilde1-2 seconds (connection timeout)
    \item \textbf{Recovery time:} \textasciitilde0.1 seconds (immediate standby query)
    \item \textbf{Data consistency:} 100\% (identical results from standby)
    \item \textbf{Availability:} Zero downtime during failover
    \item \textbf{User experience:} Transparent with warning message only
\end{itemize}

\textbf{Result:} Hot-cold standby with automatic failover successful ✓

\subsection{Monitoring Capabilities}

Figure \ref{fig:summary} shows comprehensive system monitoring.

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{screenshots/06_summary.png}
\caption{System monitoring summary showing operational status and data metrics}
\label{fig:summary}
\end{figure}

\textbf{Monitoring Features:}

\begin{enumerate}
    \item \textbf{DBMS Status Monitoring:}
    \begin{itemize}
        \item Connection status (ONLINE/OFFLINE)
        \item PostgreSQL version information
        \item Database size (DBMS1: \textasciitilde8MB, DBMS2: \textasciitilde8MB)
        \item Active connections count
    \end{itemize}

    \item \textbf{Data Distribution Monitoring:}
    \begin{itemize}
        \item Row counts per table per DBMS
        \item Total data volume
        \item Fragmentation rule verification
        \item Replication status
    \end{itemize}

    \item \textbf{Workload Monitoring:}
    \begin{itemize}
        \item Cached queries count
        \item Cache hit/miss rate
        \item Query TTL tracking
        \item Redis memory usage
    \end{itemize}

    \item \textbf{Health Monitoring:}
    \begin{itemize}
        \item Primary and standby DBMS status
        \item Failover detection and alerts
        \item Connection pool statistics
    \end{itemize}
\end{enumerate}

\textbf{Result:} All monitoring requirements met ✓

\subsection{Functional Requirements Summary}

\begin{table}[h]
\centering
\caption{Functional Requirements Verification}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Requirement} & \textbf{Implementation} & \textbf{Status} \\ \midrule
Bulk loading with partitioning & \texttt{load\_data.py} & ✓ \\
Query users/articles & \texttt{query.py execute} & ✓ \\
Distributed query processing & Router + Executor & ✓ \\
Populate Be-Read & \texttt{populate\_beread.py} & ✓ \\
Top-5 popular articles & Distributed join & ✓ \\
Data replication & Science articles, Be-Read & ✓ \\
Hot-cold standby & DBMS1-STANDBY & ✓ \\
Automatic failover & Connection error handling & ✓ \\
Query caching & Redis with 60s TTL & ✓ \\
DBMS monitoring & \texttt{monitor.py status} & ✓ \\
Distribution monitoring & \texttt{monitor.py distribution} & ✓ \\
Workload monitoring & \texttt{monitor.py workload} & ✓ \\ \bottomrule
\end{tabular}
\end{table}

\section{Conclusion}

\subsection{Summary of Achievements}

This project successfully implements a distributed database management system demonstrating core concepts of distributed data management:

\begin{enumerate}
    \item \textbf{Horizontal Fragmentation:} Implemented business-logic-based partitioning using region, category, and temporal granularity attributes with verified correct distribution

    \item \textbf{Selective Replication:} Replicated science articles and Be-Read statistics for high availability and data locality

    \item \textbf{Distributed Query Processing:} Coordinator-based architecture with intelligent routing, parallel execution, and transparent result merging

    \item \textbf{Distributed Joins:} Successfully implemented complex queries requiring joins across distributed tables (Popular-Rank + Article)

    \item \textbf{Fault Tolerance:} Hot-cold standby configuration for DBMS1 with automatic failover providing zero-downtime recovery

    \item \textbf{Query Optimization:} Redis-based caching achieving 16.67\% hit rate and 92\% response time reduction for cached queries

    \item \textbf{Comprehensive Monitoring:} Tools for DBMS health, data distribution verification, and workload analysis
\end{enumerate}

The system demonstrates that distributed database concepts can be implemented effectively using standard technologies (PostgreSQL, Redis, MinIO) with a custom coordination layer providing flexibility and control.

\subsection{Key Learnings}

\begin{itemize}
    \item \textbf{Fragmentation Strategy:} Proper fragmentation significantly impacts query performance; co-location of related tables (User + Read) reduces distributed join overhead

    \item \textbf{Replication Trade-offs:} Selective replication balances availability with storage overhead; replicating only critical data (science articles) is more efficient than full replication

    \item \textbf{Caching Effectiveness:} Query result caching provides substantial performance benefits with minimal implementation complexity; 60-second TTL balances freshness with hit rate

    \item \textbf{Coordinator Pattern:} Centralized coordinator simplifies query routing logic but requires careful failover handling; coordinator itself becomes potential single point of failure

    \item \textbf{Failover Design:} Automatic failover requires connection timeout tuning; faster detection trades off with false positives from network latency

    \item \textbf{Monitoring Importance:} Comprehensive monitoring is essential for distributed system debugging and performance optimization
\end{itemize}

\subsection{Limitations and Future Work}

\textbf{Current Limitations:}
\begin{itemize}
    \item Coordinator is single point of failure (DBMS failover implemented, coordinator failover not implemented)
    \item Manual data synchronization for standby (no automated replication)
    \item Limited query optimization (no cost-based routing)
    \item Static fragmentation rules (no dynamic rebalancing)
\end{itemize}

\textbf{Future Enhancements:}

\begin{enumerate}
    \item \textbf{Advanced Fault Tolerance:}
    \begin{itemize}
        \item Coordinator redundancy with leader election
        \item Automated PostgreSQL streaming replication
        \item DBMS2 standby configuration
        \item Automatic failback when primary recovers
    \end{itemize}

    \item \textbf{Scalability:}
    \begin{itemize}
        \item Dynamic DBMS node addition/removal
        \item Automatic data rebalancing algorithms
        \item Coordinator horizontal scaling with load balancing
        \item Partitioned caching with consistent hashing
    \end{itemize}

    \item \textbf{Query Optimization:}
    \begin{itemize}
        \item Cost-based query planning (considering network latency, DBMS load)
        \item Query result pagination for large datasets
        \item Materialized views for frequently accessed aggregations
        \item Adaptive caching with machine learning-based TTL
    \end{itemize}

    \item \textbf{Transaction Support:}
    \begin{itemize}
        \item Distributed transactions with two-phase commit (2PC)
        \item Optimistic concurrency control
        \item Deadlock detection across DBMS
    \end{itemize}

    \item \textbf{Advanced Features:}
    \begin{itemize}
        \item Real-time monitoring dashboard with Grafana
        \item Complex multi-table distributed joins (User + Read + Article)
        \item Query execution plan visualization
        \item Automated performance tuning
    \end{itemize}
\end{enumerate}

\subsection{Conclusion}

This project demonstrates that distributed database systems can be built using standard open-source technologies with careful architectural design. The implementation successfully handles horizontal fragmentation, replication, distributed query processing, fault tolerance, and caching while maintaining simplicity and understandability. The system serves as an educational platform for learning distributed database concepts and provides a foundation for more advanced features.

\section{References}

\begin{enumerate}
    \item Özsu, M. T., \& Valduriez, P. (2020). \textit{Principles of distributed database systems} (4th ed.). Springer. https://doi.org/10.1007/978-3-030-26253-2

    \item PostgreSQL Global Development Group. (2024). \textit{PostgreSQL 16 Documentation}. Retrieved from https://www.postgresql.org/docs/16/

    \item Redis Labs. (2024). \textit{Redis Documentation}. Retrieved from https://redis.io/documentation

    \item MinIO, Inc. (2024). \textit{MinIO Object Storage Documentation}. Retrieved from https://min.io/docs/

    \item Docker, Inc. (2024). \textit{Docker Documentation}. Retrieved from https://docs.docker.com/

    \item Garcia-Molina, H., Ullman, J. D., \& Widom, J. (2008). \textit{Database systems: The complete book} (2nd ed.). Pearson.

    \item Silberschatz, A., Korth, H. F., \& Sudarshan, S. (2019). \textit{Database system concepts} (7th ed.). McGraw-Hill.

    \item Tanenbaum, A. S., \& Van Steen, M. (2017). \textit{Distributed systems: Principles and paradigms} (3rd ed.). CreateSpace Independent Publishing.

    \item Kleppmann, M. (2017). \textit{Designing data-intensive applications}. O'Reilly Media.
\end{enumerate}

\newpage
\appendix

\section{Work Distribution}

This project was collaboratively developed with clear division of responsibilities:

\subsection{Henrik Kvamme (Project Lead) - 60\%}

\begin{itemize}
    \item System architecture design and component specification
    \item Query coordinator implementation:
    \begin{itemize}
        \item Query router with fragmentation rule logic
        \item Query executor with parallel execution
        \item Distributed join algorithms
    \end{itemize}
    \item Hot-cold standby and automatic failover mechanism
    \item Cache integration with Redis
    \item Project documentation (README, MANUAL, PLAN, CLAUDE.md)
    \item Report writing and technical documentation
\end{itemize}

\subsection{Rui Silveira - 40\%}

\begin{itemize}
    \item Data generation and partitioning implementation
    \item Database initialization and schema setup
    \item Be-Read population logic with aggregation
    \item Popular-Rank generation with temporal granularities
    \item Monitoring system implementation:
    \begin{itemize}
        \item DBMS status monitoring
        \item Data distribution verification
        \item Workload statistics
    \end{itemize}
    \item System testing and verification
    \item Integration testing and bug fixes
\end{itemize}

\subsection{Collaborative Work}

\begin{itemize}
    \item System design and architecture decisions
    \item Fragmentation strategy definition
    \item Integration debugging
    \item Performance testing and optimization
    \item Final system integration and deployment
\end{itemize}

\section{System Manual}

For detailed deployment instructions, command-line tool usage, and troubleshooting guide, please refer to:
\begin{itemize}
    \item \texttt{docs/MANUAL.md} - Complete system manual
    \item \texttt{docs/SCREENSHOT\_GUIDE.md} - Screenshot generation guide
    \item \texttt{README.md} - Quick start guide
    \item \texttt{CLAUDE.md} - Development guidance
\end{itemize}

\end{document}
